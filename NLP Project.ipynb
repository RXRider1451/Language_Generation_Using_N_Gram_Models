{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63274b8e",
   "metadata": {},
   "source": [
    "# Project 2 â€“ Language Generation Using N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57728e79",
   "metadata": {},
   "source": [
    "## By Derrin Naidoo (2127039)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0941292",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Derrin Naidoo\"\n",
    "STUDENT_NUMBER = \"2127039\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edbbe0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef899a",
   "metadata": {},
   "source": [
    "Note, the code was executed using a Windows 11 machine on a GPU accelerated Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00488b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from click->nltk) (4.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages (from importlib-metadata->click->nltk) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\rxrider\\anaconda3\\envs\\gpu2\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Package Installations\n",
    "\n",
    "!pip install os\n",
    "!pip install numpy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5864a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171a4e4",
   "metadata": {},
   "source": [
    "## Question 1: Read in text files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910ea74",
   "metadata": {},
   "source": [
    "Note, the variables storing the text files are: **euclid_book**, **monte_script**, **git_source** and **concat_shkspr**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43d35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in euclid elements book\n",
    "with open('project_2_data/euclid_elements/book.txt', 'r') as file:\n",
    "    euclid_book = file.read() #.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee7b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in monte python scripts\n",
    "with open('project_2_data/monte_python/scripts.txt', 'r') as file:\n",
    "    monte_script = file.read() #.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49825303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in git source file\n",
    "with open('project_2_data/git/source.c', 'r', encoding=\"utf8\") as file:\n",
    "    git_source = file.read() #.replace('\\n',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c915ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in shakespeare's files\n",
    "files = []   \n",
    "filenames = os.listdir('project_2_data/shakespeare/') #Retrieve list of file names from specified directory\n",
    "    \n",
    "#For each filename in array\n",
    "for fname in filenames:\n",
    "    #Open the file\n",
    "    file_to_open = 'project_2_data/shakespeare/' + fname \n",
    "    with open(file_to_open) as file:\n",
    "        files.append(file.read()) #Append the file to a list\n",
    "        \n",
    "#Concatenate files together into a single variable\n",
    "concat_shkspr = ''\n",
    "for file in files:\n",
    "    concat_shkspr += file\n",
    "    \n",
    "    \n",
    "#TEST CASE: Test if the concatenated shakespeare data is the same length as the sum of individual shakespeare files\n",
    "totalLength = 0\n",
    "for file in files:\n",
    "    totalLength += len(file)\n",
    "    \n",
    "assert( len(concat_shkspr) == totalLength )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83767257",
   "metadata": {},
   "source": [
    "## Question 2: Concatenate entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65711550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for concatenation: 0.008598566055297852s\n"
     ]
    }
   ],
   "source": [
    "# Variable to store concatenated dataset\n",
    "concat_data = ''\n",
    "\n",
    "start = time.time() #Start timer \n",
    "\n",
    "# Add book to dataset\n",
    "concat_data += euclid_book\n",
    "assert(len(concat_data) == len(euclid_book)) # Length of dataset should be the sum of lengths of book at this point\n",
    "\n",
    "# Add script to dataset\n",
    "concat_data += monte_script\n",
    "assert(len(concat_data) == len(euclid_book) + len(monte_script)) # Length of dataset should be the sum of lengths of book and script at this point\n",
    "\n",
    "# Add git source to dataset\n",
    "concat_data += git_source\n",
    "assert(len(concat_data) == len(euclid_book) + len(monte_script) + len(git_source)) # Length of dataset should be the sum of lengths of book, script and git source at this point\n",
    "\n",
    "#Add shakespeare to dataset\n",
    "concat_data += concat_shkspr\n",
    "assert(len(concat_data) == len(euclid_book) + len(monte_script) + len(git_source) + len(concat_shkspr)) # Length of dataset should be the sum of all individual datasets at this point\n",
    "\n",
    "end = time.time() #End timer \n",
    "\n",
    "#Show time taken to produce concatenated dataset\n",
    "print(f'Time taken for concatenation: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3130e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated dataset contains 11261284 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Concatenated dataset contains {len(concat_data)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80638d67",
   "metadata": {},
   "source": [
    "## Question 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3328f",
   "metadata": {},
   "source": [
    "### 3.1 Lowercase Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe6b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a peice of text and converts the entire set to lowercase characters only\n",
    "def textToLower(text):\n",
    "    lower_text = '' #Have to copy list so we don't alter the original array parameter\n",
    "    for i in range(len(text)):\n",
    "        lower_text += text[i].lower()\n",
    "    return lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa483ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Case: Ensure that the textToLower function works correctly\n",
    "test = 'AaBbCcDdFf'\n",
    "assert(textToLower(test) == 'aabbccddff')\n",
    "\n",
    "test = 'a  B  D  f'\n",
    "assert(textToLower(test) == 'a  b  d  f')\n",
    "\n",
    "test = 'sA[]##$FP)'\n",
    "assert(textToLower(test) == 'sa[]##$fp)')\n",
    "\n",
    "test = '10A9 4s8G'\n",
    "assert(textToLower(test) == '10a9 4s8g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf536b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for lowercase conversion: 26.070250749588013s\n"
     ]
    }
   ],
   "source": [
    "#Create a version of each dataset with only lowercase characters\n",
    "\n",
    "start = time.time() #Start timer \n",
    "\n",
    "lower_euclid_book = textToLower(euclid_book)\n",
    "assert(len(euclid_book) == len(lower_euclid_book)) #Ensure the number of chars in dataset hasn't changed\n",
    "\n",
    "lower_monte_script = textToLower(monte_script)\n",
    "assert(len(monte_script) == len(lower_monte_script)) #Ensure the number of chars in dataset hasn't changed\n",
    "\n",
    "lower_git_source = textToLower(git_source)\n",
    "assert(len(git_source) == len(lower_git_source)) #Ensure the number of chars in dataset hasn't changed\n",
    "\n",
    "lower_concat_shkspr = textToLower(concat_shkspr)\n",
    "assert(len(concat_shkspr) == len(lower_concat_shkspr)) #Ensure the number of chars in dataset hasn't changed\n",
    "\n",
    "lower_concat_data = textToLower(concat_data)\n",
    "assert(len(concat_data) == len(lower_concat_data)) #Ensure the number of chars in dataset hasn't changed\n",
    "\n",
    "end = time.time() #End timer \n",
    "\n",
    "#Show time taken to produce lowercase datasets\n",
    "print(f'Time taken for lowercase conversion: {end-start}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0639d89",
   "metadata": {},
   "source": [
    "### 3.2 Obtain Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf884f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclid Elements book has 3014 sentences\n",
      "\n",
      "Monte Python scripts has 15524 sentences\n",
      "\n",
      "Git Source has 19444 sentences\n",
      "\n",
      "Concatenated Shakespeare has 35936 sentences\n",
      "\n",
      "Concatenated data has 73915 sentences\n",
      "\n",
      "Time taken for sentence extraction: 0.047324419021606445s\n"
     ]
    }
   ],
   "source": [
    "#Obtain sentences for each dataset by splitting each lowercase dataset by sequence of '\\n\\n' characters\n",
    "\n",
    "start = time.time() #Start timer \n",
    "\n",
    "sen_euclid_book = lower_euclid_book.split('\\n\\n')\n",
    "print(f'Euclid Elements book has {len(sen_euclid_book)} sentences\\n')\n",
    "\n",
    "sen_monte_script = lower_monte_script.split('\\n\\n')\n",
    "print(f'Monte Python scripts has {len(sen_monte_script)} sentences\\n')\n",
    "\n",
    "sen_git_source = lower_git_source.split('\\n\\n')\n",
    "print(f'Git Source has {len(sen_git_source)} sentences\\n')\n",
    "\n",
    "sen_concat_shkspr = lower_concat_shkspr.split('\\n\\n')\n",
    "print(f'Concatenated Shakespeare has {len(sen_concat_shkspr)} sentences\\n')\n",
    "\n",
    "sen_concat_data = lower_concat_data.split('\\n\\n')\n",
    "print(f'Concatenated data has {len(sen_concat_data)} sentences\\n')\n",
    "\n",
    "\n",
    "end = time.time() #End timer \n",
    "\n",
    "#Show time taken to produce lowercase datasets\n",
    "print(f'Time taken for sentence extraction: {end-start}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3458c7",
   "metadata": {},
   "source": [
    "### 3.3 Tokenise Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45729e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return an array of words from a sentence\n",
    "def tokenisedSentence(sentence, split_char=''):\n",
    "    if split_char=='':\n",
    "        words = sentence.split()\n",
    "    else:\n",
    "        words = sentence.split(split_char)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac17608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to obtain tokenised datasets: 0.35183024406433105s\n"
     ]
    }
   ],
   "source": [
    "start = time.time() #Start timer \n",
    "\n",
    "processed_euclid_book = []\n",
    "for sentence in sen_euclid_book:\n",
    "    processed_euclid_book.append(tokenisedSentence(sentence))\n",
    "\n",
    "processed_monte_script = []\n",
    "for sentence in sen_monte_script:\n",
    "    processed_monte_script.append(tokenisedSentence(sentence))\n",
    "\n",
    "processed_git_source = []\n",
    "for sentence in sen_git_source:\n",
    "    processed_git_source.append(tokenisedSentence(sentence))    \n",
    "\n",
    "processed_concat_shkspr = []\n",
    "for sentence in sen_concat_shkspr:\n",
    "    processed_concat_shkspr.append(tokenisedSentence(sentence))\n",
    "\n",
    "processed_concat_data = []\n",
    "for sentence in sen_concat_data:\n",
    "    processed_concat_data.append(tokenisedSentence(sentence))\n",
    "\n",
    "end = time.time() #End timer \n",
    "\n",
    "#Show time taken to produce tokenised datasets\n",
    "print(f'Time taken to obtain tokenised datasets: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3673257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preprocessed datasets to numpy arrays\n",
    "# Note, after all preprocessing, the dataset variables are renamed to the original name given upon reading files (Question 1)\n",
    "# and, each sentence is still of type list, but the entire array of sentences is a numpy array \n",
    "\n",
    "euclid_book = np.array(processed_euclid_book, dtype=object)\n",
    "monte_script = np.array(processed_monte_script, dtype=object)\n",
    "git_source = np.array(processed_git_source, dtype=object)\n",
    "concat_shkspr = np.array(processed_concat_shkspr, dtype=object)\n",
    "concat_data = np.array(processed_concat_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6760a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Case: Tokenised datasets should have the same number of sentences before tokenisation\n",
    "\n",
    "assert(len(sen_euclid_book) == euclid_book.shape[0])\n",
    "assert(len(sen_monte_script) == monte_script.shape[0])\n",
    "assert(len(sen_git_source) == git_source.shape[0])\n",
    "assert(len(sen_concat_shkspr) == concat_shkspr.shape[0])\n",
    "assert(len(sen_concat_data) == concat_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7a102",
   "metadata": {},
   "source": [
    "## Question 4: Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba7ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return a trained language model based on a certain n_gram and dataset\n",
    "def trainLanguageModel(n_gram, data):\n",
    "    \n",
    "    # Initialise language model based on Maximum Likelihood Estimator with a certain n_gram\n",
    "    languageModel = MLE(n_gram) \n",
    "    \n",
    "    #Preprocess data into an everygram with <s> and </s> padding\n",
    "    train, vocab = padded_everygram_pipeline(2, data)\n",
    "    \n",
    "    #Fit the language model to the training data and vocabulary\n",
    "    languageModel.fit(train, vocab)\n",
    "    \n",
    "    return languageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3054ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Training Euclid Book Dataset (1/5) =========================\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 2-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 3-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 4-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "========================= Training Monte Scripts Dataset (2/5) =========================\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 2-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 3-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 4-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "========================= Training Git Source Dataset (3/5) =========================\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 2-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 3-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 4-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "========================= Training Concatenated Shakespeare Dataset (4/5) =========================\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 2-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 3-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 4-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "========================= Training Concatenated Data Dataset (5/5) =========================\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 2-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 3-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained 4-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "Time taken for language model training: 2.6113938689231873mins\n"
     ]
    }
   ],
   "source": [
    "n_grams = [2,3,4] # Stores n-grams to train\n",
    "all_datasets = [euclid_book, monte_script, git_source, concat_shkspr, concat_data] # Stores datasets \n",
    "dataset_names = [\"Euclid Book\",\"Monte Scripts\",\"Git Source\",\"Concatenated Shakespeare\",\"Concatenated Data\"] # Dataset names \n",
    "trained_lms = [] #Stores all trained n-gram language models\n",
    "\n",
    "# ============================== Train Language Models ==============================\n",
    "start = time.time() #Start timer \n",
    "\n",
    "#For each dataset\n",
    "for i in range(len(all_datasets)):\n",
    "    print(f\"========================= Training {dataset_names[i]} Dataset ({i+1}/{len(all_datasets)}) =========================\")\n",
    "    \n",
    "    #For each n-gram\n",
    "    for n_gram in n_grams:\n",
    "        trained_lms.append(trainLanguageModel(n_gram, all_datasets[i])) #Append trained language model to list\n",
    "        print(f\"<<<<<<<<<<<<<<<<<<<<<<<<<<< Trained {n_gram}-Gram Language Model >>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "end = time.time() #End timer \n",
    "# ===================================================================================\n",
    "\n",
    "#Show time taken to produce lowercase datasets\n",
    "print(f'Time taken for language model training: {(end-start)/60}mins')\n",
    "\n",
    "\n",
    "#Test Case: Ensure we've obtained 15 language models after training\n",
    "assert(len(trained_lms) == 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcf47c",
   "metadata": {},
   "source": [
    "#### Brief Description\n",
    "\n",
    "At this point, we've trained 15 language models and appended them to the **trained_lms** list.<br>\n",
    "\n",
    "Each N-Gram in the list follows eachother **consequetively** and is in the order: **2-Gram, 3-Gram and 4-Gram**<br>\n",
    "The order of the list as per each index is as follows:<br><br>\n",
    "\n",
    "**0,1,2**: Euclid Book Dataset<br>\n",
    "**3,4,5**: Monte Scripts Dataset<br>\n",
    "**6,7,8**: Git Source Dataset<br>\n",
    "**9,10,11**: Concatenated Shakespeare Dataset<br>\n",
    "**12,13,14**: Concatenated Dataset<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d96de",
   "metadata": {},
   "source": [
    "## Question 5: Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f2c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to formate a sentence from a list into a line of text where words are seperated by empty characters\n",
    "def formattedSentence(sentence_list):\n",
    "    text = '' #Initilaise text to empty string\n",
    "    \n",
    "    #For each word of the sentence\n",
    "    for i in range(len(sentence_list)):\n",
    "        text += sentence_list[i]\n",
    "        #Add an empty character if it's not the last word of the sentence\n",
    "        if i+1 < len(sentence_list):\n",
    "            text += ' '\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f5c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a sentence given the formatted list of trained language models and set parameters\n",
    "#A description of parameters if given below:\n",
    "# words_per_sentence: boolean which specifies the number of words in a generated sentence\n",
    "# formatted_sentence: boolean which specifies whether the printed sentence is formatted or not\n",
    "# random_seed: specifies boolean which whether the seed for each sentence should be the same or different\n",
    "# seeds: list which contains seeds for each sentence that's produced (must be of length NUM_SENTENCES)\n",
    "\n",
    "def generateSentences(trained_lms, words_per_sentence=10, formatted_sentence = True, random_seed=True, seeds = None):\n",
    "    \n",
    "    \n",
    "    # ===== Constants =====\n",
    "    NUM_SENTENCES = 30\n",
    "    \n",
    "    #Range to generate random numbers\n",
    "    LOWER_RANGE = 1\n",
    "    UPPER_RANGE = 1000\n",
    "    \n",
    "    # =====================\n",
    "    \n",
    "    # ==================== Generating/Assigning Seeds for Sentences ====================\n",
    "\n",
    "    #Initialise random_seeds array\n",
    "    sentence_seeds = []\n",
    "    \n",
    "    #If we want a random seed to be chosen\n",
    "    if seeds == None:\n",
    "        #If we want the same seed for every sentence\n",
    "        if random_seed == False:\n",
    "            #Using the same seed for each sentence, but the seed is chosen randomly\n",
    "            random_num = random.randint(LOWER_RANGE, UPPER_RANGE)\n",
    "            sentence_seeds = [random_num for i in range(30)]\n",
    "        \n",
    "        #If we want a different seed for every sentence (where seeds are not repeated)\n",
    "        else:\n",
    "            while len(sentence_seeds) != 30:\n",
    "                random_num = random.randint(LOWER_RANGE, UPPER_RANGE)\n",
    "                if random_num not in sentence_seeds:\n",
    "                    sentence_seeds.append(random_num)\n",
    "                    \n",
    "    #If we're using the seeds passed into the function\n",
    "    else:\n",
    "        assert(len(seeds) == NUM_SENTENCES) # Must have a seed for every sentence\n",
    "        sentence_seeds = seeds    \n",
    "        \n",
    "    # ==================================================================================\n",
    "\n",
    "    # Store each language model in a dictionary for the relevent dataset\n",
    "    dict_data = {}\n",
    "    dict_data[\"Euclid Book\"] = [trained_lms[i] for i in [0,1,2]]\n",
    "    dict_data[\"Monte Scripts\"] = [trained_lms[i] for i in [3,4,5]]\n",
    "    dict_data[\"Git Source\"] = [trained_lms[i] for i in [6,7,8]]\n",
    "    dict_data[\"Concatenated Shakespeare\"] = [trained_lms[i] for i in [9,10,11]]\n",
    "    dict_data[\"Concatenated Data\"] = [trained_lms[i] for i in [12,13,14]]\n",
    "    \n",
    "    \n",
    "    # Print Metadata for generating sentences\n",
    "    print('\\033[1;3m= = = = = = = = = = = = = = = = = = = Metadata = = = = = = = = = = = = = = = = = = =\\033[0m')\n",
    "    print(f'Words per sentence: {words_per_sentence}')\n",
    "    print(f'Number of Sentences: {NUM_SENTENCES}')\n",
    "    if random_seed == False:\n",
    "        print(f'Seed: {sentence_seeds[0]}')\n",
    "    else:\n",
    "        print(f'Seeds: {sentence_seeds}')\n",
    "        \n",
    "    print('\\033[1;3m= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \\033[0m')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "    # ==================== Generating and Printing Sentences ====================\n",
    "    \n",
    "    start = time.time() #Start timer\n",
    "    \n",
    "    #For each dataset\n",
    "    for key in dict_data:\n",
    "        \n",
    "        print(f'\\033[1;3m================================ {key} Sentences ================================\\033[0m')\n",
    "    \n",
    "        n_gram_counter = 2\n",
    "        \n",
    "        #For each language model (each n-gram)\n",
    "        for lm in dict_data[key]:\n",
    "            print(f'<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< {n_gram_counter}-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n')\n",
    "            start_words = []\n",
    "            #For each of the 2 sentences\n",
    "            for i in range(2):\n",
    "                \n",
    "                #Choose a start word that's not already chosen for this N-Gram\n",
    "                start_word = np.random.choice(list(lm.vocab))\n",
    "                counter = 0\n",
    "                #Keep generating sentences until a sentence starts with a different start word\n",
    "                while start_word in start_words and counter < 99999:\n",
    "                    start_word = np.random.choice(list(lm.vocab))\n",
    "                    counter += 1 # Alternate escape condition\n",
    "                start_words.append(start_word)\n",
    "                \n",
    "                print(f'(Start Word: {start_word})')\n",
    "                #Generate a sentence\n",
    "                generated_sentence = lm.generate(words_per_sentence, text_seed=[start_word], random_seed = sentence_seeds.pop())\n",
    "                \n",
    "                #Print sentence\n",
    "                if formatted_sentence == True:\n",
    "                    print(formattedSentence(generated_sentence))\n",
    "                    print('\\n')\n",
    "                else:\n",
    "                    print(generated_sentence)\n",
    "                    print('\\n')\n",
    "            n_gram_counter += 1\n",
    "\n",
    "    end = time.time() #End timer \n",
    "    \n",
    "    # ===========================================================================\n",
    "\n",
    "    #Show time taken to generate sentences\n",
    "    print(f'Time taken to generate sentences: {end-start}s')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "612308b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3m= = = = = = = = = = = = = = = = = = = Metadata = = = = = = = = = = = = = = = = = = =\u001b[0m\n",
      "Words per sentence: 10\n",
      "Number of Sentences: 30\n",
      "Seed: 533\n",
      "\u001b[1;3m= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;3m================================ Euclid Book Sentences ================================\u001b[0m\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 2-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: thus:-)\n",
      "['1409f086', '</s>', 'any', 'part', 'between', 'the', 'circle', 'described', 'on', 'this']\n",
      "\n",
      "\n",
      "(Start Word: english)\n",
      "['computer,', 'carried', 'as', 'i.', '</s>', 'convex', 'or', 'less', 'than', 'two']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 3-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: remainder.)\n",
      "['</s>', 'present', 'day-the', 'production', 'note', 'b.', '&&', '</s>', 'now', 'the']\n",
      "\n",
      "\n",
      "(Start Word: ratio.)\n",
      "['</s>', 'present', 'day-the', 'production', 'note', 'b.', '&&', '</s>', 'now', 'the']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 4-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: use)\n",
      "['of', 'the', 'difference', 'is', 'a', 'given', 'circle;', 'also', 'much', 'of']\n",
      "\n",
      "\n",
      "(Start Word: direction;)\n",
      "['in', 'the', 'difference', 'is', 'a', 'given', 'circle;', 'also', 'much', 'of']\n",
      "\n",
      "\n",
      "\u001b[1;3m================================ Monte Scripts Sentences ================================\u001b[0m\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 2-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: foams)\n",
      "['at', 'the', 'door', 'is', 'a', 'handful...', '</s>', 'floe', 'and', 'welcome']\n",
      "\n",
      "\n",
      "(Start Word: tinkling)\n",
      "['with', 'rubber', 'gloves', 'and', 'a', 'handful...', '</s>', 'floe', 'and', 'welcome']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 3-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: sideboard...)\n",
      "['yes.', \"i'm\", 'beginning', 'to', 'a', 'handful...', '</s>', 'floe', 'and', 'welcome']\n",
      "\n",
      "\n",
      "(Start Word: dense)\n",
      "['undergrowth', 'by', 'do', 'tend', 'to', 'give', 'it', 'is', 'on', 'wall']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 4-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: receiver)\n",
      "['at', 'the', 'door', 'is', 'a', 'handful...', '</s>', 'floe', 'and', 'welcome']\n",
      "\n",
      "\n",
      "(Start Word: ruislip...)\n",
      "['</s>', 'palin):', 'harry', 'bagot.', '</s>', 'emporium.', '</s>', 'floe', 'and', 'welcome']\n",
      "\n",
      "\n",
      "\u001b[1;3m================================ Git Source Sentences ================================\u001b[0m\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 2-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: cb->recno++;)\n",
      "['return', 'n;', 'i++)', '{', '.refname', '=', '0;', 'i', '<', 'store->seen_nr']\n",
      "\n",
      "\n",
      "(Start Word: strlen(deltaenv[i]))\n",
      "['*', 'rebased,', 'and', 'have', '*', 'dwim_ref()', 'uses', 'it', 'is', 'unique']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 3-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: rename_target.)\n",
      "['*/', 'return', '0;', 'i', '+=', 'exclude_per_dir_offset', '+', '1]', '==', 'subtop->alasts,']\n",
      "\n",
      "\n",
      "(Start Word: max_generation)\n",
      "['=', 'p->two->path;', 'else', 'if', '(!cur_state->has_constraint', '||', '!strcmp(value,', '\"inherit\"))', '{', 'uint8_t']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 4-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: found.nr;)\n",
      "['i++)', '{', 'const', 'char', '**name_p)', '{', 'char', '*in)', '{', 'uint8_t']\n",
      "\n",
      "\n",
      "(Start Word: *)lookup_tree(r,)\n",
      "['&oid);', '}', '</s>', 'c->host', '=', 'ci->stages[1].mode;', 'oidcpy(&ci->merged.result.oid,', '&ci->stages[index].oid);', 'ci->merged.clean', '=']\n",
      "\n",
      "\n",
      "\u001b[1;3m================================ Concatenated Shakespeare Sentences ================================\u001b[0m\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 2-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: sovereignty.)\n",
      "['</s>', 'reflection,', 'by', 'my', 'bounty', 'had', 'before', 'marriage.', '[they', 'smear']\n",
      "\n",
      "\n",
      "(Start Word: shoulder!)\n",
      "['fame', \"o'ershine\", 'you', 'hurt', 'and', 'i', 'come,', \"here's\", 'my', 'voice,']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 3-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: boy,\")\n",
      "['say', 'their', 'encounter', 'mine.', '</s>', 'great', 'business', 'in', 'smithfield', 'to']\n",
      "\n",
      "\n",
      "(Start Word: begin,)\n",
      "['as', 'subject', 'his', 'heart', 'and', 'i', 'come,', \"here's\", 'my', 'voice,']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 4-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: dunghill.)\n",
      "['</s>', 'reflection,', 'by', 'my', 'bounty', 'had', 'before', 'marriage.', '[they', 'smear']\n",
      "\n",
      "\n",
      "(Start Word: perish)\n",
      "['here.', 'i', 'do', 'me', 'alone.', '</s>', 'been', 'godlike', 'reason', 'with']\n",
      "\n",
      "\n",
      "\u001b[1;3m================================ Concatenated Data Sentences ================================\u001b[0m\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 2-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: certain,)\n",
      "['for', 'ten', \"nights'\", 'watchings.', '</s>', 'edward', 'clifford,', 'for', \"others'\", 'waning,']\n",
      "\n",
      "\n",
      "(Start Word: lour'st)\n",
      "['on', 'the', \"empress'\", 'love.', '</s>', 'edward', 'clifford,', 'for', \"others'\", 'waning,']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 3-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: is_inside_git_dir()))\n",
      "['return;', '}', '</s>', 'give', 'any', 'lions', 'by', 'malignant', 'death,', 'who']\n",
      "\n",
      "\n",
      "(Start Word: default_packet_max)\n",
      "['or', 'rename/copy', 'detection', 'if', '(!eos', '||', '!strcmp(value,', '\"inherit\"))', '{', 'uint8_t']\n",
      "\n",
      "\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 4-Gram >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "(Start Word: is_parent;)\n",
      "['int', 'parse_config(unsigned', 'int', 'i;', '</s>', 'edward', 'clifford,', 'for', \"others'\", 'waning,']\n",
      "\n",
      "\n",
      "(Start Word: *kept_up)\n",
      "['=', 'p->pathlen;', 'newinfo.mode', '=', \"'_';\", '}', '</s>', 'file', 'is', 'unwonted']\n",
      "\n",
      "\n",
      "Time taken to generate sentences: 10.162929773330688s\n"
     ]
    }
   ],
   "source": [
    "#Generate Sentences\n",
    "generateSentences(trained_lms, formatted_sentence = False, random_seed = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b8866",
   "metadata": {},
   "source": [
    "## Question 6: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada188f",
   "metadata": {},
   "source": [
    "We observe through the results that the ability of a language model to generate coherent sentences is dependent on the size of the training corpus as well as the size of the n-gram. The sentences generated by 2-Gram language models sound less coherent which indicates that the model lacks contextual information which would allow the sentences to sound more natural. This lack of context means that the language model tends to generalise its sentences. However, we see that while 2-grams have limited context, the increased size of training corpora allows for more context even amongst lower-sized n-grams. We observe this particularly in the **Concatenated Shakespeare** 2-gram sentences (large training corpus) which sound more coherent compared to the **Euclid Elements book** 2-gram sentences (smaller training corpus). The disparity in training size accounts for this ability to generate natural sentences or lack thereof. Consequently, we observe that in the **Concatenated Shakespeare** 4-gram sentences, the text seems quite natural at first glance due to the combination of large training data and increased contextual information for words. Importantly, if the contexts in training corpora should differ to a large degree, as in the **Concatenated Dataset** case where the training corpus is a combination of all other datasets, the language model aims to fit all the included data which means sentences will either be very general or exhibit bias towards a single dataset. In our case, the datasets contexts are significantly different (i.e. Shakespeare, code, textbook, etc), therefore, we observe that the sentences tend to form bias towards datasets where the start words and subsequent words are uniquely found. In particular, we observe a bias towards the **Git Source** dataset as words in that dataset tend to be uniquely found in there amongst the other datasets which means that these sentences infequently contain words from the other datasets. As a final note, we also observe that occasionally even with different start words, the generated sentences for lower sized n-grams may be identical which infers that the the probability distribution of a word occuring may be equal given different contexts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
